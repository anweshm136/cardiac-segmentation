{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('base': conda)",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dicom'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3b4cce0b4618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdicom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dicom'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, glob, re\n",
    "import dicom\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "from math import ceil\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from keras import utils\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_rotate(image):\n",
    "    # orient image in landscape\n",
    "    height, width = image.shape\n",
    "    return np.rot90(image) if width < height else image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientData(object):\n",
    "    \"\"\"Data directory structure (for patient 01):\n",
    "    directory/\n",
    "      P01dicom.txt\n",
    "      P01dicom/\n",
    "        P01-0000.dcm\n",
    "        P01-0001.dcm\n",
    "        ...\n",
    "      P01contours-manual/\n",
    "        P01-0080-icontour-manual.txt\n",
    "        P01-0120-ocontour-manual.txt\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, directory):\n",
    "        self.directory = os.path.normpath(directory)\n",
    "\n",
    "        # get patient index from contour listing file\n",
    "        glob_search = os.path.join(directory, \"P*list.txt\")\n",
    "        files = glob.glob(glob_search)\n",
    "\n",
    "        self.contour_list_file = files[0]\n",
    "        match = re.search(\"P(..)list.txt\", self.contour_list_file)\n",
    "        self.index = int(match.group(1))\n",
    "\n",
    "        # load all data into memory\n",
    "        self.load_images()\n",
    "\n",
    "        # some patients do not have contour data, and that's ok\n",
    "        try:\n",
    "            self.load_masks()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return [self.all_images[i] for i in self.labeled]\n",
    "\n",
    "    @property\n",
    "    def dicoms(self):\n",
    "        return [self.all_dicoms[i] for i in self.labeled]\n",
    "\n",
    "    @property\n",
    "    def dicom_path(self):\n",
    "        return os.path.join(self.directory, \"P{:02d}dicom\".format(self.index))\n",
    "\n",
    "    def load_images(self):\n",
    "        glob_search = os.path.join(self.dicom_path, \"*.dcm\")\n",
    "        dicom_files = sorted(glob.glob(glob_search))\n",
    "        self.all_images = []\n",
    "        self.all_dicoms = []\n",
    "        for dicom_file in dicom_files:\n",
    "            plan = dicom.read_file(dicom_file)\n",
    "            image = maybe_rotate(plan.pixel_array)\n",
    "            self.all_images.append(image)\n",
    "            self.all_dicoms.append(plan)\n",
    "        self.image_height, self.image_width = image.shape\n",
    "        self.rotated = (plan.pixel_array.shape != image.shape)\n",
    "\n",
    "    def load_contour(self, filename):\n",
    "        # strip out path head \"patientXX/\"\n",
    "        match = re.search(\"patient../(.*)\", filename)\n",
    "        path = os.path.join(self.directory, match.group(1))\n",
    "        x, y = np.loadtxt(path).T\n",
    "        if self.rotated:\n",
    "            x, y = y, self.image_height - x\n",
    "        return x, y\n",
    "\n",
    "    def contour_to_mask(self, x, y, norm=255):\n",
    "        BW_8BIT = 'L'\n",
    "        polygon = list(zip(x, y))\n",
    "        image_dims = (self.image_width, self.image_height)\n",
    "        img = Image.new(BW_8BIT, image_dims, color=0)\n",
    "        ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
    "        return norm * np.array(img, dtype='uint8')\n",
    "\n",
    "    def load_masks(self):\n",
    "        with open(self.contour_list_file, 'r') as f:\n",
    "            files = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        inner_files = [path.replace(\"\\\\\", \"/\") for path in files[0::2]]\n",
    "        outer_files = [path.replace(\"\\\\\", \"/\") for path in files[1::2]]\n",
    "\n",
    "        # get list of frames which have contours\n",
    "        self.labeled = []\n",
    "        for inner_file in inner_files:\n",
    "            match = re.search(\"P..-(....)-.contour\", inner_file)\n",
    "            frame_number = int(match.group(1))\n",
    "            self.labeled.append(frame_number)\n",
    "\n",
    "        self.endocardium_contours = []\n",
    "        self.epicardium_contours = []\n",
    "        self.endocardium_masks = []\n",
    "        self.epicardium_masks = []\n",
    "        for inner_file, outer_file in zip(inner_files, outer_files):\n",
    "            inner_x, inner_y = self.load_contour(inner_file)\n",
    "            self.endocardium_contours.append((inner_x, inner_y))\n",
    "            outer_x, outer_y = self.load_contour(outer_file)\n",
    "            self.epicardium_contours.append((outer_x, outer_y))\n",
    "\n",
    "            inner_mask = self.contour_to_mask(inner_x, inner_y, norm=1)\n",
    "            self.endocardium_masks.append(inner_mask)\n",
    "            outer_mask = self.contour_to_mask(outer_x, outer_y, norm=1)\n",
    "            self.epicardium_masks.append(outer_mask)\n",
    "    \"\"\"        \n",
    "    def write_video(self, outfile, FPS=24):\n",
    "        import cv2\n",
    "        image_dims = (self.image_width, self.image_height)\n",
    "        video = cv2.VideoWriter(outfile, -1, FPS, image_dims)\n",
    "        for image in self.all_images:\n",
    "            grayscale = np.asarray(image * (255 / image.max()), dtype='uint8')\n",
    "            video.write(cv2.cvtColor(grayscale, cv2.COLOR_GRAY2BGR))\n",
    "        video.release()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(data_dir, mask='both'):\n",
    "    \"\"\"Load all patient images and contours from TrainingSet, Test1Set or\n",
    "    Test2Set directory. The directories and images are read in sorted order.\n",
    "\n",
    "    Arguments:\n",
    "      data_dir - path to data directory (TrainingSet, Test1Set or Test2Set)\n",
    "\n",
    "    Output:\n",
    "      tuples of (images, masks), both of which are 4-d tensors of shape\n",
    "      (batchsize, height, width, channels). Images is uint16 and masks are\n",
    "      uint8 with values 0 or 1.\n",
    "    \"\"\"\n",
    "    assert mask in ['inner', 'outer', 'both']\n",
    "\n",
    "    glob_search = os.path.join(data_dir, \"patient*\")\n",
    "    patient_dirs = sorted(glob.glob(glob_search))\n",
    "\n",
    "    # load all images into memory (dataset is small)\n",
    "    images = []\n",
    "    inner_masks = []\n",
    "    outer_masks = []\n",
    "    for patient_dir in patient_dirs:\n",
    "        p = patient.PatientData(patient_dir)\n",
    "        images += p.images\n",
    "        inner_masks += p.endocardium_masks\n",
    "        outer_masks += p.epicardium_masks\n",
    "\n",
    "    # reshape to account for channel dimension\n",
    "    images = np.asarray(images)[:,:,:,None]\n",
    "    if mask == 'inner':\n",
    "        masks = np.asarray(inner_masks)\n",
    "    elif mask == 'outer':\n",
    "        masks = np.asarray(outer_masks)\n",
    "    elif mask == 'both':\n",
    "        # mask = 2 for endocardium, 1 for cardiac wall, 0 elsewhere\n",
    "        masks = np.asarray(inner_masks) + np.asarray(outer_masks)\n",
    "\n",
    "    # one-hot encode masks\n",
    "    dims = masks.shape\n",
    "    classes = len(set(masks[0].flatten())) # get num classes from first image\n",
    "    new_shape = dims + (classes,)\n",
    "    masks = utils.to_categorical(masks).reshape(new_shape)\n",
    "\n",
    "    return images, masks\n",
    "\"\"\"\n",
    "def random_elastic_deformation(image, alpha, sigma, mode='nearest',\n",
    "                               random_state=None):\n",
    "    Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \n",
    "    assert len(image.shape) == 3\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    dx = gaussian_filter(2*random_state.rand(height, width) - 1,\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter(2*random_state.rand(height, width) - 1,\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "    indices = (np.repeat(np.ravel(x+dx), channels),\n",
    "               np.repeat(np.ravel(y+dy), channels),\n",
    "               np.tile(np.arange(channels), height*width))\n",
    "    \n",
    "    values = map_coordinates(image, indices, order=1, mode=mode)\n",
    "\n",
    "    return values.reshape((height, width, channels))\n",
    "\"\"\"\n",
    "class Iterator(object):\n",
    "    def __init__(self, images, masks, batch_size,\n",
    "                 shuffle=True,\n",
    "                 rotation_range=180,\n",
    "                 width_shift_range=0.1,\n",
    "                 height_shift_range=0.1,\n",
    "                 shear_range=0.1,\n",
    "                 zoom_range=0.01,\n",
    "                 fill_mode='nearest',\n",
    "                 alpha=500,\n",
    "                 sigma=20):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        augment_options = {\n",
    "            'rotation_range': rotation_range,\n",
    "            'width_shift_range': width_shift_range,\n",
    "            'height_shift_range': height_shift_range,\n",
    "            'shear_range': shear_range,\n",
    "            'zoom_range': zoom_range,\n",
    "            'fill_mode': fill_mode,\n",
    "        }\n",
    "        self.idg = ImageDataGenerator(**augment_options)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.fill_mode = fill_mode\n",
    "        self.i = 0\n",
    "        self.index = np.arange(len(images))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        # compute how many images to output in this batch\n",
    "        start = self.i\n",
    "        end = min(start + self.batch_size, len(self.images))\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_masks = []\n",
    "        for n in self.index[start:end]:\n",
    "            image = self.images[n]\n",
    "            mask = self.masks[n]\n",
    "\n",
    "            _, _, channels = image.shape\n",
    "\n",
    "            # stack image + mask together to simultaneously augment\n",
    "            stacked = np.concatenate((image, mask), axis=2)\n",
    "\n",
    "            # apply simple affine transforms first using Keras\n",
    "            augmented = self.idg.random_transform(stacked)\n",
    "            \"\"\"\n",
    "            # maybe apply elastic deformation\n",
    "            if self.alpha != 0 and self.sigma != 0:\n",
    "                augmented = random_elastic_deformation(\n",
    "                    augmented, self.alpha, self.sigma, self.fill_mode)\n",
    "            \"\"\"\n",
    "\n",
    "            # split image and mask back apart\n",
    "            augmented_image = augmented[:,:,:channels]\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_mask = np.round(augmented[:,:,channels:])\n",
    "            augmented_masks.append(augmented_mask)\n",
    "\n",
    "        self.i += self.batch_size\n",
    "        if self.i >= len(self.images):\n",
    "            self.i = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.index)\n",
    "\n",
    "        return np.asarray(augmented_images), np.asarray(augmented_masks)\n",
    "\n",
    "def normalize(x, epsilon=1e-7, axis=(1,2)):\n",
    "    x -= np.mean(x, axis=axis, keepdims=True)\n",
    "    x /= np.std(x, axis=axis, keepdims=True) + epsilon\n",
    "\n",
    "def create_generators(data_dir, batch_size, validation_split=0.0, mask='both',\n",
    "                      shuffle_train_val=True, shuffle=True, seed=None,\n",
    "                      normalize_images=True, augment_training=False,\n",
    "                      augment_validation=False, augmentation_args={}):\n",
    "    images, masks = load_images(data_dir, mask)\n",
    "\n",
    "    # before: type(masks) = uint8 and type(images) = uint16\n",
    "    # convert images to double-precision\n",
    "    images = images.astype('float64')\n",
    "\n",
    "    # maybe normalize image\n",
    "    if normalize_images:\n",
    "        normalize(images, axis=(1,2))\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if shuffle_train_val:\n",
    "        # shuffle images and masks in parallel\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(images)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(masks)\n",
    "\n",
    "    # split out last %(validation_split) of images as validation set\n",
    "    split_index = int((1-validation_split) * len(images))\n",
    "\n",
    "    if augment_training:\n",
    "        train_generator = Iterator(\n",
    "            images[:split_index], masks[:split_index],\n",
    "            batch_size, shuffle=shuffle, **augmentation_args)\n",
    "    else:\n",
    "        idg = ImageDataGenerator()\n",
    "        train_generator = idg.flow(images[:split_index], masks[:split_index],\n",
    "                                   batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    train_steps_per_epoch = ceil(split_index / batch_size)\n",
    "\n",
    "    if validation_split > 0.0:\n",
    "        if augment_validation:\n",
    "            val_generator = Iterator(\n",
    "                images[split_index:], masks[split_index:],\n",
    "                batch_size, shuffle=shuffle, **augmentation_args)\n",
    "        else:\n",
    "            idg = ImageDataGenerator()\n",
    "            val_generator = idg.flow(images[split_index:], masks[split_index:],\n",
    "                                     batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        val_generator = None\n",
    "\n",
    "    val_steps_per_epoch = ceil((len(images) - split_index) / batch_size)\n",
    "\n",
    "    return (train_generator, train_steps_per_epoch,\n",
    "            val_generator, val_steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_sorensen_dice(y_true, y_pred, axis=None, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=axis)\n",
    "    area_true = K.sum(y_true, axis=axis)\n",
    "    area_pred = K.sum(y_pred, axis=axis)\n",
    "    return (2 * intersection + smooth) / (area_true + area_pred + smooth)\n",
    "\n",
    "def sorensen_dice_loss(y_true, y_pred, weights):\n",
    "    # Input tensors have shape (batch_size, height, width, classes)\n",
    "    # User must input list of weights with length equal to number of classes\n",
    "    #\n",
    "    # Ex: for simple binary classification, with the 0th mask\n",
    "    # corresponding to the background and the 1st mask corresponding\n",
    "    # to the object of interest, we set weights = [0, 1]\n",
    "    batch_dice_coefs = soft_sorensen_dice(y_true, y_pred, axis=[1, 2])\n",
    "    dice_coefs = K.mean(batch_dice_coefs, axis=0)\n",
    "    w = K.constant(weights) / sum(weights)\n",
    "    return 1 - K.sum(w * dice_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimizer(optimizer_name, optimizer_args):\n",
    "    optimizers = {\n",
    "        'sgd': SGD,\n",
    "        'rmsprop': RMSprop,\n",
    "        'adagrad': Adagrad,\n",
    "        'adadelta': Adadelta,\n",
    "        'adam': Adam,\n",
    "        'adamax': Adamax,\n",
    "        'nadam': Nadam,\n",
    "    }\n",
    "\n",
    "    return optimizers[optimizer_name](**optimizer_args)\n",
    "\n",
    "def train():\n",
    "    #logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    #args = opts.parse_arguments()\n",
    "\n",
    "    #logging.info(\"Loading dataset...\")\n",
    "    augmentation_args = {\n",
    "        'rotation_range': args.rotation_range,\n",
    "        'width_shift_range': args.width_shift_range,\n",
    "        'height_shift_range': args.height_shift_range,\n",
    "        'shear_range': args.shear_range,\n",
    "        'zoom_range': args.zoom_range,\n",
    "        'fill_mode' : args.fill_mode,\n",
    "        'alpha': args.alpha,\n",
    "        'sigma': args.sigma,\n",
    "    }\n",
    "    train_generator, train_steps_per_epoch, \\\n",
    "        val_generator, val_steps_per_epoch = dataset.create_generators(\n",
    "            args.datadir, args.batch_size,\n",
    "            validation_split=args.validation_split,\n",
    "            mask=args.classes,\n",
    "            shuffle_train_val=args.shuffle_train_val,\n",
    "            shuffle=args.shuffle,\n",
    "            seed=args.seed,\n",
    "            normalize_images=args.normalize,\n",
    "            augment_training=args.augment_training,\n",
    "            augment_validation=args.augment_validation,\n",
    "            augmentation_args=augmentation_args)\n",
    "\n",
    "    # get image dimensions from first batch\n",
    "    images, masks = next(train_generator)\n",
    "    _, height, width, channels = images.shape\n",
    "    _, _, _, classes = masks.shape\n",
    "\n",
    "    logging.info(\"Building model...\")\n",
    "    string_to_model = {\n",
    "        \"unet\": models.unet,\n",
    "    }\n",
    "    model = string_to_model[args.model]\n",
    "    m = model(height=height, width=width, channels=channels, classes=classes,\n",
    "              features=32, depth=3, padding='same',\n",
    "              temperature=1.0, batchnorm=False,\n",
    "              dropout=0.0)\n",
    "\n",
    "    m.summary()\n",
    "\"\"\"\n",
    "    if args.load_weights:\n",
    "        logging.info(\"Loading saved weights from file: {}\".format(args.load_weights))\n",
    "        m.load_weights(args.load_weights)\n",
    "\"\"\"\n",
    "    # instantiate optimizer, and only keep args that have been set\n",
    "    # (not all optimizers have args like `momentum' or `decay')\n",
    "    optimizer_args = {\n",
    "        'lr':       None,\n",
    "        'momentum': None,\n",
    "        'decay':    None\n",
    "    }\n",
    "    for k in list(optimizer_args):\n",
    "        if optimizer_args[k] is None:\n",
    "            del optimizer_args[k]\n",
    "    optimizer = select_optimizer('adam', optimizer_args)\n",
    "\n",
    "    # select loss function: pixel-wise crossentropy, soft dice or soft\n",
    "    if args.loss == 'pixel':\n",
    "        def lossfunc(y_true, y_pred):\n",
    "            return loss.weighted_categorical_crossentropy(\n",
    "                y_true, y_pred, args.loss_weights)\n",
    "    elif args.loss == 'dice':\n",
    "        def lossfunc(y_true, y_pred):\n",
    "            return loss.sorensen_dice_loss(y_true, y_pred, args.loss_weights)\n",
    "\n",
    "    def dice(y_true, y_pred):\n",
    "        batch_dice_coefs = loss.sorensen_dice(y_true, y_pred, axis=[1, 2])\n",
    "        dice_coefs = K.mean(batch_dice_coefs, axis=0)\n",
    "        return dice_coefs[1]    # HACK for 2-class case\n",
    "\n",
    "    metrics = ['accuracy', dice]\n",
    "\n",
    "    m.compile(optimizer=optimizer, loss=lossfunc, metrics=metrics)\n",
    "\n",
    "    # automatic saving of model during training\n",
    "    if args.checkpoint:\n",
    "        if args.loss == 'pixel':\n",
    "            filepath = os.path.join(\n",
    "                args.outdir, \"weights-{epoch:02d}-{val_acc:.4f}.hdf5\")\n",
    "            monitor = 'val_acc'\n",
    "            mode = 'max'\n",
    "        elif args.loss == 'dice':\n",
    "            filepath = os.path.join(\n",
    "                args.outdir, \"weights-{epoch:02d}-{val_dice:.4f}.hdf5\")\n",
    "            monitor='val_dice'\n",
    "            mode = 'max'\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath, monitor=monitor, verbose=1,\n",
    "            save_best_only=True, mode=mode)\n",
    "        callbacks = [checkpoint]\n",
    "    else:\n",
    "        callbacks = []\n",
    "\n",
    "    # train\n",
    "    logging.info(\"Begin training.\")\n",
    "    m.fit_generator(train_generator,\n",
    "                    epochs=args.epochs,\n",
    "                    steps_per_epoch=train_steps_per_epoch,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=2)\n",
    "\n",
    "    m.save(os.path.join(args.outdir, args.outfile))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  }
 ]
}